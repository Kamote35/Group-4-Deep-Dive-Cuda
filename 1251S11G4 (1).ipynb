{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cb0486-f5a9-4690-a031-2792ac2d01e3",
   "metadata": {},
   "source": [
    "# Setup CUDA Directory Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22187016-d57e-4082-93af-7d9a998a3143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/bin:/bin:/usr/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory containing the executable to the PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin\"\n",
    "\n",
    "# Check if the directory is added to the PATH\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb0f5d-8afa-4d0d-9101-b3663b1f8e73",
   "metadata": {},
   "source": [
    "## C program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e7daa9-0a6f-403a-ad56-9a434d1b41da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C_mvp.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile C_mvp.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "void c_version(float* A, float* x, float* y, int m, int n) {\n",
    "    int i, j;\n",
    "\n",
    "    // The result vector 'y' must be initialized to zero before summation.\n",
    "    for (i = 0; i < m; i++) {\n",
    "        y[i] = 0.0f;\n",
    "    }\n",
    "\n",
    "    // Standard Matrix-Vector Multiplication loop structure:\n",
    "    // y_i = sum_j ( A_ij * x_j )\n",
    "    for (i = 0; i < m; i++) { // Loop over rows of A (index for y)\n",
    "        float sum = 0.0f;\n",
    "        for (j = 0; j < n; j++) { // Loop over columns of A (index for x)\n",
    "            // A[i * n + j] accesses the element A_ij in row-major order\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    A = (float*)malloc(MATRIX_BYTES);\n",
    "    x = (float*)malloc(VECTOR_BYTES);\n",
    "    y = (float*)malloc(VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure \n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        // Use fprintf(stderr, ...) like daxpy\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        // Free any that might have succeeded\n",
    "        free(A);\n",
    "        free(x);\n",
    "        free(y);\n",
    "        return 1; // Exit with an error code\n",
    "    }\n",
    "\n",
    "  //timer variables\n",
    "    clock_t start, end;\n",
    "\n",
    "    // Initialize Matrix A:\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)...\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            // A[i * n + j] = 1.0 / (i + j + 1.0)\n",
    "            A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Initialize Vector x:\n",
    "    printf(\"Initializing Vector x (%lu elements)...\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "\n",
    "    // fill-in cache\n",
    "    c_version(A, x, y, M, N);\n",
    "\n",
    "    // time here***\n",
    "    double elapse = 0.0, time_taken;\n",
    "    \n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        start = clock();\n",
    "        c_version(A, x, y, M, N);\n",
    "        end = clock();\n",
    "        time_taken = ((double)(end - start)) * 1E3 / CLOCKS_PER_SEC;\n",
    "        elapse += time_taken;\n",
    "    }\n",
    "\n",
    "    printf(\"\\n--- Benchmark Results ---\\n\");\n",
    "    printf(\"Function c_version (in C) average time for %lu loops is %f milliseconds for matrix (m=%lu, n=%lu)\\n\", \n",
    "           loope, elapse / (double)loope, M, N);\n",
    "\n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // error checking routine here -- \n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ // For each element in y\n",
    "        // Recalculate the correct value for y[i]\n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (C program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    free(A);\n",
    "    free(x);\n",
    "    free(y);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d96a32c8-601e-4b3c-ac85-17363f019794",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcc C_mvp.c -o C_mvp -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "252f611b-db08-48d4-9eec-442f2038ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "--- Benchmark Results ---\n",
      "Function c_version (in C) average time for 30 loops is 2078.384433 milliseconds for matrix (m=16384, n=16384)\n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (C program): 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./C_mvp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaae536-6c30-4a54-a28c-8a8cb563b925",
   "metadata": {},
   "source": [
    "## CUDA Grid-stride loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eebe629-b4fb-416c-ab95-4aa6164a3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_mvp2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp2.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    // Allocate managed memory \n",
    "    cudaMallocManaged(&A, MATRIX_BYTES);\n",
    "    cudaMallocManaged(&x, VECTOR_BYTES);\n",
    "    cudaMallocManaged(&y, VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure\n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        cudaFree(A);\n",
    "        cudaFree(x);\n",
    "        cudaFree(y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    // ***--- initialize your array here ---------\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)...\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "    printf(\"Initializing Vector x (%lu elements)...\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "    // *** setup CUDA kernel\n",
    "    size_t numThreads = 1024; \n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(A, x, y, M, N);\n",
    "    }\n",
    "\n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "\n",
    "    // error checking routine here \n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    cudaFree(A);\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    \n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec296e17-cc33-4a1c-9411-53c93c6371b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp2.cu -o CUDA_mvp2 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "559d50ce-41c1-406e-8f1f-08a4bea766da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1047855== NVPROF is profiling process 1047855, command: ./CUDA_mvp2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1047855== Profiling application: ./CUDA_mvp2\n",
      "==1047855== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  1.79134s        30  59.711ms  42.401ms  484.03ms  mvp_kernel(float const *, float const *, float*, unsigned long, unsigned long)\n",
      "      API calls:   52.16%  2.08889s         3  696.30ms  198.69us  2.08756s  cudaMallocManaged\n",
      "                   44.79%  1.79343s         1  1.79343s  1.79343s  1.79343s  cudaDeviceSynchronize\n",
      "                    2.73%  109.37ms         3  36.457ms  656.90us  107.73ms  cudaFree\n",
      "                    0.27%  10.679ms        30  355.98us  13.560us  9.9582ms  cudaLaunchKernel\n",
      "                    0.04%  1.4876ms       114  13.048us     163ns  587.47us  cuDeviceGetAttribute\n",
      "                    0.01%  423.36us         1  423.36us  423.36us  423.36us  cuDeviceGetName\n",
      "                    0.00%  69.786us         1  69.786us  69.786us  69.786us  cuDeviceTotalMem\n",
      "                    0.00%  24.045us         1  24.045us  24.045us  24.045us  cuDeviceGetPCIBusId\n",
      "                    0.00%  16.050us         3  5.3500us     232ns  12.992us  cuDeviceGetCount\n",
      "                    0.00%  15.074us         2  7.5370us     413ns  14.661us  cuDeviceGet\n",
      "                    0.00%  1.3420us         1  1.3420us  1.3420us  1.3420us  cuDeviceGetUuid\n",
      "                    0.00%  1.1810us         1  1.1810us  1.1810us  1.1810us  cuModuleGetLoadingMode\n",
      "\n",
      "==1047855== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "   15216  38.864KB  4.0000KB  0.9961MB  577.5117MB  165.9177ms  Host To Device\n",
      "    6148  170.58KB  4.0000KB  0.9961MB  1.000122GB  728.5885ms  Device To Host\n",
      "      97         -         -         -           -  316.4793ms  Gpu page fault groups\n",
      "Total CPU Page faults: 6147\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c85a3b-357b-45b8-b900-fc43e6b890e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: nsys: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'nsys profile  -o CUDA_mvp2 ./CUDA_mvp2\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnsys profile  -o CUDA_mvp2 ./CUDA_mvp2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tljh/user/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tljh/user/lib/python3.12/site-packages/IPython/core/magics/script.py:159\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tljh/user/lib/python3.12/site-packages/IPython/core/magics/script.py:336\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    335\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'nsys profile  -o CUDA_mvp2 ./CUDA_mvp2\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp2 ./CUDA_mvp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf5a9c-da71-4199-a535-345da8d65336",
   "metadata": {},
   "source": [
    "## CUDA Grid-stride loop with prefetching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b523bbc9-0cfd-40e5-a337-001bf07065b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_mvp3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp3.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    // Allocate managed memory \n",
    "    cudaMallocManaged(&A, MATRIX_BYTES);\n",
    "    cudaMallocManaged(&x, VECTOR_BYTES);\n",
    "    cudaMallocManaged(&y, VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure\n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        cudaFree(A);\n",
    "        cudaFree(x);\n",
    "        cudaFree(y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    //get gpu id\n",
    "    int device = -1;\n",
    "    cudaGetDevice(&device);\n",
    "    \n",
    "    // ***--- initialize your array here ---------\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)...\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "    printf(\"Initializing Vector x (%lu elements)...\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "    //\"Prefetch data\" from CPU-GPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, device, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, device, NULL);\n",
    "    \n",
    "    // *** setup CUDA kernel\n",
    "    size_t numThreads = 1024; \n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(A, x, y, M, N);\n",
    "    }\n",
    "\n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    //\"Prefetch data\" from GPU-CPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(y, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "\n",
    "    // --- Display first 3 and last 3 elements \n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // error checking routine here \n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    cudaFree(A);\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    \n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62666ef7-31b9-409f-ac93-a9b7c3ad1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp3.cu -o CUDA_mvp3 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144f5c30-e3be-4b6f-9abc-8ec7c51bd7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1060552== NVPROF is profiling process 1060552, command: ./CUDA_mvp3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1060552== Profiling application: ./CUDA_mvp3\n",
      "==1060552== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  866.34ms        30  28.878ms  27.642ms  30.216ms  mvp_kernel(float const *, float const *, float*, unsigned long, unsigned long)\n",
      "      API calls:   55.84%  1.97716s         3  659.05ms  22.848us  1.97659s  cudaMallocManaged\n",
      "                   24.54%  868.90ms         1  868.90ms  868.90ms  868.90ms  cudaDeviceSynchronize\n",
      "                   15.49%  548.39ms         5  109.68ms  124.60us  437.56ms  cudaMemPrefetchAsync\n",
      "                    2.60%  92.084ms         3  30.695ms  384.29us  90.848ms  cudaFree\n",
      "                    1.48%  52.479ms        30  1.7493ms  11.521us  51.744ms  cudaLaunchKernel\n",
      "                    0.03%  1.0209ms       114  8.9550us     136ns  395.08us  cuDeviceGetAttribute\n",
      "                    0.01%  383.52us         1  383.52us  383.52us  383.52us  cuDeviceGetName\n",
      "                    0.00%  97.167us         2  48.583us     301ns  96.866us  cuDeviceGet\n",
      "                    0.00%  40.192us         1  40.192us  40.192us  40.192us  cuDeviceTotalMem\n",
      "                    0.00%  38.528us         1  38.528us  38.528us  38.528us  cuDeviceGetPCIBusId\n",
      "                    0.00%  14.688us         1  14.688us  14.688us  14.688us  cudaGetDevice\n",
      "                    0.00%  10.248us         3  3.4160us     328ns  8.0580us  cuDeviceGetCount\n",
      "                    0.00%  2.6340us         1  2.6340us  2.6340us  2.6340us  cuModuleGetLoadingMode\n",
      "                    0.00%     755ns         1     755ns     755ns     755ns  cuDeviceGetUuid\n",
      "\n",
      "==1060552== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     513  1.9962MB  64.000KB  2.0000MB  1.000061GB  151.9514ms  Host To Device\n",
      "     514  1.9925MB  64.000KB  2.0000MB  1.000122GB  429.6826ms  Device To Host\n",
      "       1         -         -         -           -  992.3340us  Gpu page fault groups\n",
      "Total CPU Page faults: 3073\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442fd00a-f2e7-43e2-953a-761df52fbcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to create '/home/jupyter-gerald_corpuz@dlsu-94f82/CUDA_mvp3.nsys-rep': File exists.\n",
      "Use `--force-overwrite true` to overwrite existing files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-b846.qdstrm'\n",
      "[1/1] [========================100%] nsys-report-6055.nsys-rep\n",
      "Generated:\n",
      "\t/tmp/nsys-report-6055.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp3 ./CUDA_mvp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af208204-251f-4355-a79f-1053412a16f7",
   "metadata": {},
   "source": [
    "### CUDA Grid-stride loop with prefetch and page creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c59c5b00-84a3-4aa4-a6df-ef1c7ef952ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_mvp4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp4.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    // Allocate managed memory \n",
    "    cudaMallocManaged(&A, MATRIX_BYTES);\n",
    "    cudaMallocManaged(&x, VECTOR_BYTES);\n",
    "    cudaMallocManaged(&y, VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure\n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        cudaFree(A);\n",
    "        cudaFree(x);\n",
    "        cudaFree(y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    //get gpu id\n",
    "    int device = -1;\n",
    "    cudaGetDevice(&device);\n",
    "\n",
    "    //\"prefetch data\" to create CPU page memory\n",
    "    cudaMemPrefetchAsync(A,MATRIX_BYTES,cudaCpuDeviceId,NULL);\n",
    "    cudaMemPrefetchAsync(x,VECTOR_BYTES,cudaCpuDeviceId,NULL);\n",
    "    //\"prefetch data\" to create GPU page memory\n",
    "    cudaMemPrefetchAsync(y,VECTOR_BYTES,device,NULL);\n",
    "\n",
    "    // ***--- initialize your array here ---------\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)...\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "    printf(\"Initializing Vector x (%lu elements)...\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "    //\"Prefetch data\" from CPU-GPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, device, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, device, NULL);\n",
    "    \n",
    "    // *** setup CUDA kernel\n",
    "    size_t numThreads = 1024; \n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(A, x, y, M, N);\n",
    "    }\n",
    "\n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    //\"Prefetch data\" from GPU-CPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(y, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "\n",
    "    // --- Display first 3 and last 3 elements \n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // error checking routine here \n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    cudaFree(A);\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    \n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9822e8b-2bea-4406-95e3-8f51011f5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp4.cu -o CUDA_mvp4 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36f353b9-916d-4231-b183-f757f5f49c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1048884== NVPROF is profiling process 1048884, command: ./CUDA_mvp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1048884== Profiling application: ./CUDA_mvp4\n",
      "==1048884== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  858.08ms        30  28.603ms  13.258ms  246.49ms  mvp_kernel(float const *, float const *, float*, unsigned long, unsigned long)\n",
      "      API calls:   37.21%  1.73487s         8  216.86ms  268.88us  1.19062s  cudaMemPrefetchAsync\n",
      "                   35.44%  1.65244s         3  550.81ms  67.413us  1.65152s  cudaMallocManaged\n",
      "                   19.73%  919.92ms         1  919.92ms  919.92ms  919.92ms  cudaDeviceSynchronize\n",
      "                    3.82%  178.08ms        30  5.9361ms  19.024us  176.98ms  cudaLaunchKernel\n",
      "                    3.77%  175.62ms         3  58.540ms  728.49us  172.52ms  cudaFree\n",
      "                    0.03%  1.3627ms       114  11.953us     129ns  576.11us  cuDeviceGetAttribute\n",
      "                    0.01%  523.28us         1  523.28us  523.28us  523.28us  cuDeviceGetName\n",
      "                    0.00%  90.773us         1  90.773us  90.773us  90.773us  cuDeviceTotalMem\n",
      "                    0.00%  26.410us         1  26.410us  26.410us  26.410us  cudaGetDevice\n",
      "                    0.00%  20.225us         1  20.225us  20.225us  20.225us  cuDeviceGetPCIBusId\n",
      "                    0.00%  12.463us         3  4.1540us     242ns  9.0510us  cuDeviceGetCount\n",
      "                    0.00%  5.7830us         2  2.8910us     720ns  5.0630us  cuDeviceGet\n",
      "                    0.00%  3.8250us         1  3.8250us  3.8250us  3.8250us  cuDeviceGetUuid\n",
      "                    0.00%  2.4650us         1  2.4650us  2.4650us  2.4650us  cuModuleGetLoadingMode\n",
      "\n",
      "==1048884== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     513  1.9962MB  64.000KB  2.0000MB  1.000061GB  182.4054ms  Host To Device\n",
      "     514  1.9925MB  64.000KB  2.0000MB  1.000122GB  425.4770ms  Device To Host\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "892a7011-ee5b-4168-a10a-ee47365e1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-6cff.qdstrm'\n",
      "[1/1] [========================100%] CUDA_mvp4.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-gerald_corpuz@dlsu-94f82/CUDA_mvp4.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp4 ./CUDA_mvp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1e7e-e8fe-4c29-9465-4b1cd7620c95",
   "metadata": {},
   "source": [
    "### CUDA Grid-stride loop with prefetch and page creation and mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c929a7ac-206a-4bd4-b9a1-4332c2bb379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_mvp5.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp5.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    // Allocate managed memory \n",
    "    cudaMallocManaged(&A, MATRIX_BYTES);\n",
    "    cudaMallocManaged(&x, VECTOR_BYTES);\n",
    "    cudaMallocManaged(&y, VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure\n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        cudaFree(A);\n",
    "        cudaFree(x);\n",
    "        cudaFree(y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    //get gpu id\n",
    "    int device = -1;\n",
    "    cudaGetDevice(&device);\n",
    "\n",
    "    // memory advise\n",
    "    cudaMemAdvise(A, MATRIX_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
    "    cudaMemAdvise(A, MATRIX_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
    "    cudaMemAdvise(x, VECTOR_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
    "    cudaMemAdvise(x, VECTOR_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
    "\n",
    "    //\"prefetch data\" to create CPU page memory\n",
    "    cudaMemPrefetchAsync(A,MATRIX_BYTES,cudaCpuDeviceId,NULL);\n",
    "    cudaMemPrefetchAsync(x,VECTOR_BYTES,cudaCpuDeviceId,NULL);\n",
    "    //\"prefetch data\" to create GPU page memory\n",
    "    cudaMemPrefetchAsync(y,VECTOR_BYTES,device,NULL);\n",
    "\n",
    "    // ***--- initialize your array here ---------\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)...\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "    printf(\"Initializing Vector x (%lu elements)...\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "    //\"Prefetch data\" from CPU-GPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, device, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, device, NULL);\n",
    "    \n",
    "    // *** setup CUDA kernel\n",
    "    size_t numThreads = 1024; \n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(A, x, y, M, N);\n",
    "    }\n",
    "    \n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    //\"Prefetch data\" from GPU-CPU\n",
    "    cudaMemPrefetchAsync(A, MATRIX_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(x, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "    cudaMemPrefetchAsync(y, VECTOR_BYTES, cudaCpuDeviceId, NULL);\n",
    "\n",
    "    // --- Display first 3 and last 3 elements \n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // error checking routine here \n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    cudaFree(A);\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e126e98-b64d-42e8-a663-7e38d64d5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp5.cu -o CUDA_mvp5 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c263ea22-32d6-4db0-9094-22b0c63a1250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1049512== NVPROF is profiling process 1049512, command: ./CUDA_mvp5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1049512== Profiling application: ./CUDA_mvp5\n",
      "==1049512== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  431.93ms        30  14.398ms  13.257ms  14.873ms  mvp_kernel(float const *, float const *, float*, unsigned long, unsigned long)\n",
      "      API calls:   50.53%  2.19424s         3  731.41ms  81.789us  2.19316s  cudaMallocManaged\n",
      "                   37.36%  1.62221s         8  202.78ms  619.46us  1.35592s  cudaMemPrefetchAsync\n",
      "                    9.93%  431.12ms         1  431.12ms  431.12ms  431.12ms  cudaDeviceSynchronize\n",
      "                    2.03%  88.136ms         3  29.379ms  722.90us  86.625ms  cudaFree\n",
      "                    0.11%  4.5782ms        30  152.60us  14.801us  3.6960ms  cudaLaunchKernel\n",
      "                    0.03%  1.3738ms       114  12.050us     127ns  549.41us  cuDeviceGetAttribute\n",
      "                    0.01%  379.43us         1  379.43us  379.43us  379.43us  cuDeviceGetName\n",
      "                    0.01%  313.93us         4  78.482us  10.191us  253.00us  cudaMemAdvise\n",
      "                    0.00%  80.351us         1  80.351us  80.351us  80.351us  cuDeviceTotalMem\n",
      "                    0.00%  38.625us         1  38.625us  38.625us  38.625us  cudaGetDevice\n",
      "                    0.00%  14.500us         2  7.2500us     478ns  14.022us  cuDeviceGet\n",
      "                    0.00%  11.299us         1  11.299us  11.299us  11.299us  cuDeviceGetPCIBusId\n",
      "                    0.00%  6.1960us         3  2.0650us     159ns  5.6710us  cuDeviceGetCount\n",
      "                    0.00%  1.7200us         1  1.7200us  1.7200us  1.7200us  cuDeviceGetUuid\n",
      "                    0.00%  1.4660us         1  1.4660us  1.4660us  1.4660us  cuModuleGetLoadingMode\n",
      "\n",
      "==1049512== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "     513  1.9962MB  64.000KB  2.0000MB  1.000061GB  183.5342ms  Host To Device\n",
      "       1  64.000KB  64.000KB  64.000KB  64.00000KB  49.18400us  Device To Host\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5fb3614-ae24-47de-a8ca-0d0a59de7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)...\n",
      "Initializing Vector x (16384 elements)...\n",
      "Initialization complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-e8a4.qdstrm'\n",
      "[1/1] [========================100%] CUDA_mvp5.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-gerald_corpuz@dlsu-94f82/CUDA_mvp5.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp5 ./CUDA_mvp5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38624f9-ef9a-4fa3-a663-6168eeecd205",
   "metadata": {},
   "source": [
    "### Classic MemCopy method (no Unified memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7f0494-a260-4e0e-8caa-6d7ba2001fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_mvp6.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp6.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare host arrays \n",
    "    float *h_A, *h_x, *h_y;\n",
    "    //declare device arrays\n",
    "    float *d_A, *d_x, *d_y;\n",
    "\n",
    "    // Allocate host memory \n",
    "    h_A = (float*)malloc(MATRIX_BYTES);\n",
    "    h_x = (float*)malloc(VECTOR_BYTES);\n",
    "    h_y = (float*)malloc(VECTOR_BYTES);\n",
    "\n",
    "    if (h_A == NULL || h_x == NULL || h_y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate host memory for matrix/vectors.\\n\");\n",
    "        free(h_A);\n",
    "        free(h_x);\n",
    "        free(h_y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    // Allocate device memory\n",
    "    cudaMalloc(&d_A, MATRIX_BYTES);\n",
    "    cudaMalloc(&d_x, VECTOR_BYTES);\n",
    "    cudaMalloc(&d_y, VECTOR_BYTES);\n",
    "    \n",
    "    if (d_A == NULL || d_x == NULL || d_y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate device memory for matrix/vectors.\\n\");\n",
    "        cudaFree(d_A);\n",
    "        cudaFree(d_x);\n",
    "        cudaFree(d_y);\n",
    "        free(h_A);\n",
    "        free(h_x);\n",
    "        free(h_y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "\n",
    "    // ***--- initialize your array on the HOST here --------\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)... on host\\n\", MATRIX_SIZE, M, N);\n",
    "    for (size_t i = 0; i < M; i++) {\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            h_A[i * N + j] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "        }\n",
    "    }\n",
    "    printf(\"Initializing Vector x (%lu elements)... on host\\n\", VECTOR_SIZE);\n",
    "    for (size_t j = 0; j < N; j++) {\n",
    "        h_x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "    // *** Copy data from Host to Device\n",
    "    printf(\"Copying data from Host to Device...\\n\");\n",
    "    cudaMemcpy(d_A, h_A, MATRIX_BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_x, h_x, VECTOR_BYTES, cudaMemcpyHostToDevice);\n",
    "    printf(\"Copy complete.\\n\");\n",
    "\n",
    "    // *** setup CUDA kernel\n",
    "    size_t numThreads = 1024; \n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA - Classic MemCpy)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        // Launch kernel with DEVICE pointers\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(d_A, d_x, d_y, M, N);\n",
    "    }\n",
    "\n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Kernel execution complete.\\n\");\n",
    "\n",
    "    // *** Copy result from Device to Host\n",
    "    printf(\"Copying result from Device to Host...\\n\");\n",
    "    cudaMemcpy(h_y, d_y, VECTOR_BYTES, cudaMemcpyDeviceToHost);\n",
    "    printf(\"Copy complete.\\n\");\n",
    "\n",
    "\n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements (from host array)\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, h_y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements (from host array)\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, h_y[i]);\n",
    "    }\n",
    "\n",
    "\n",
    "    // error checking routine here (using host arrays)\n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += h_A[i * N + j] * h_x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(h_y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free device memory \n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "    \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_x);\n",
    "    free(h_y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e394d8-5222-435f-a2aa-2afd9f5736a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp6.cu -o CUDA_mvp6 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119a5657-8572-40b1-b523-13e2aa13878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1065633== NVPROF is profiling process 1065633, command: ./CUDA_mvp6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)... on host\n",
      "Initializing Vector x (16384 elements)... on host\n",
      "Initialization complete.\n",
      "Copying data from Host to Device...\n",
      "Copy complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA - Classic MemCpy)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "Kernel execution complete.\n",
      "Copying result from Device to Host...\n",
      "Copy complete.\n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1065633== Profiling application: ./CUDA_mvp6\n",
      "==1065633== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   77.77%  1.48308s         2  741.54ms  12.928us  1.48306s  [CUDA memcpy HtoD]\n",
      "                   22.22%  423.80ms        30  14.127ms  13.408ms  14.833ms  mvp_kernel(float const *, float const *, float*, unsigned long, unsigned long)\n",
      "                    0.00%  20.095us         1  20.095us  20.095us  20.095us  [CUDA memcpy DtoH]\n",
      "      API calls:   47.69%  1.90540s         3  635.13ms  16.899us  1.90463s  cudaMalloc\n",
      "                   37.20%  1.48623s         3  495.41ms  419.03us  1.48472s  cudaMemcpy\n",
      "                   10.59%  423.15ms         1  423.15ms  423.15ms  423.15ms  cudaDeviceSynchronize\n",
      "                    4.41%  176.14ms         3  58.712ms  306.34us  172.83ms  cudaFree\n",
      "                    0.09%  3.5588ms        30  118.63us  17.666us  2.8505ms  cudaLaunchKernel\n",
      "                    0.02%  827.94us       114  7.2620us     151ns  314.24us  cuDeviceGetAttribute\n",
      "                    0.01%  327.81us         1  327.81us  327.81us  327.81us  cuDeviceGetName\n",
      "                    0.00%  41.985us         1  41.985us  41.985us  41.985us  cuDeviceTotalMem\n",
      "                    0.00%  16.097us         1  16.097us  16.097us  16.097us  cuDeviceGetPCIBusId\n",
      "                    0.00%  14.885us         2  7.4420us     370ns  14.515us  cuDeviceGet\n",
      "                    0.00%  7.0000us         3  2.3330us     739ns  4.2380us  cuDeviceGetCount\n",
      "                    0.00%  2.5290us         1  2.5290us  2.5290us  2.5290us  cuDeviceGetUuid\n",
      "                    0.00%     337ns         1     337ns     337ns     337ns  cuModuleGetLoadingMode\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bb4421-0bca-414a-b6c5-a6c6f4a1f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Matrix A (268435456 elements, 16384 x 16384)... on host\n",
      "Initializing Vector x (16384 elements)... on host\n",
      "Initialization complete.\n",
      "Copying data from Host to Device...\n",
      "Copy complete.\n",
      "\n",
      "*** function = Matrix Vector Product (CUDA - Classic MemCpy)\n",
      "Matrix (m x n) = 16384 x 16384\n",
      "numBlocks = 16, numThreads = 1024 \n",
      "Kernel execution complete.\n",
      "Copying result from Device to Host...\n",
      "Copy complete.\n",
      "\n",
      "--- Output Vector Y (m=16384) ---\n",
      "First 3 elements:\n",
      "y[0] = 11.800177\n",
      "y[1] = 10.766556\n",
      "y[2] = 10.237693\n",
      "\n",
      "Last 3 elements:\n",
      "y[16381] = 0.703410\n",
      "y[16382] = 0.703379\n",
      "y[16383] = 0.703348\n",
      "\n",
      "Starting error check...\n",
      "Error count (CUDA program): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to create '/home/jupyter-gerald_corpuz@dlsu-94f82/CUDA_mvp6.nsys-rep': File exists.\n",
      "Use `--force-overwrite true` to overwrite existing files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-31b7.qdstrm'\n",
      "[1/1] [========================100%] nsys-report-bc7e.nsys-rep\n",
      "Generated:\n",
      "\t/tmp/nsys-report-bc7e.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp6 ./CUDA_mvp6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605526a8-82b1-45b2-b7eb-df3e3c2a1445",
   "metadata": {},
   "source": [
    "###  CUDA Kernel with Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18722075-a2d5-49a9-bcee-f21e83d083d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CUDA_mvp7.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_mvp7.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "__global__\n",
    "void init_A_kernel(float* A, size_t M, size_t N)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "    size_t total_elements = M * N;\n",
    "\n",
    "    for (size_t idx = index; idx < total_elements; idx += stride)\n",
    "    {\n",
    "        size_t i = idx / N; // Get row\n",
    "        size_t j = idx % N; // Get col\n",
    "        // A[idx] is the same as A[i * N + j]\n",
    "        A[idx] = 1.0f / ((float)i + (float)j + 1.0f);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__\n",
    "void init_x_kernel(float* x, size_t N)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockIdx.x * gridDim.x;\n",
    "\n",
    "    for (size_t j = index; j < N; j += stride)\n",
    "    {\n",
    "        x[j] = sinf((float)j * 0.01f) * cosf((float)j * 0.007f) + 1.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__global__\n",
    "void mvp_kernel(const float* A, const float* x, float* y, size_t m, size_t n)\n",
    "{\n",
    "    size_t index = (size_t)blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    size_t stride = (size_t)blockDim.x * gridDim.x;\n",
    "\n",
    "    for (size_t i = index; i < m; i += stride) {\n",
    "        float sum = 0.0f;\n",
    "        for (size_t j = 0; j < n; j++) {\n",
    "            // A[i * n + j] accesses the element A_ij\n",
    "            sum += A[i * n + j] * x[j];\n",
    "        }\n",
    "        y[i] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    const size_t M = 1 << 14; \n",
    "    const size_t N = M;       \n",
    "\n",
    "    const size_t MATRIX_SIZE = M * N; \n",
    "    const size_t VECTOR_SIZE = N;     \n",
    "\n",
    "    const size_t MATRIX_BYTES = MATRIX_SIZE * sizeof(float);\n",
    "    const size_t VECTOR_BYTES = VECTOR_SIZE * sizeof(float);\n",
    "\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    //declare array \n",
    "    float *A, *x, *y;\n",
    "\n",
    "    // Allocate managed memory \n",
    "    cudaMallocManaged(&A, MATRIX_BYTES);\n",
    "    cudaMallocManaged(&x, VECTOR_BYTES);\n",
    "    cudaMallocManaged(&y, VECTOR_BYTES);\n",
    "\n",
    "    // Check for allocation failure\n",
    "    if (A == NULL || x == NULL || y == NULL) {\n",
    "        fprintf(stderr, \"Failed to allocate memory for matrix/vectors.\\n\");\n",
    "        cudaFree(A);\n",
    "        cudaFree(x);\n",
    "        cudaFree(y);\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    // *** setup CUDA kernel launch parameters\n",
    "    size_t numThreads = 1024; \n",
    "    \n",
    "    // ***--- initialize your array here using CUDA Kernels ---\n",
    "    printf(\"Initializing Matrix A (%lu elements, %lu x %lu)... on GPU\\n\", MATRIX_SIZE, M, N);\n",
    "    size_t total_elements_A = M * N;\n",
    "    size_t numBlocks_A = (total_elements_A + numThreads - 1) / numThreads;\n",
    "    init_A_kernel<<<numBlocks_A, numThreads>>>(A, M, N);\n",
    "    \n",
    "    printf(\"Initializing Vector x (%lu elements)... on GPU\\n\", VECTOR_SIZE);\n",
    "    size_t numBlocks_x = (N + numThreads - 1) / numThreads;\n",
    "    init_x_kernel<<<numBlocks_x, numThreads>>>(x, N);\n",
    "\n",
    "    // Synchronize to ensure initialization is complete\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Initialization complete.\\n\");\n",
    "\n",
    "\n",
    "    // *** setup CUDA kernel for MVP\n",
    "    size_t numBlocks = (M + numThreads - 1) / numThreads;\n",
    "\n",
    "    printf(\"\\n*** function = Matrix Vector Product (CUDA - Kernel Init)\\n\");\n",
    "    printf(\"Matrix (m x n) = %lu x %lu\\n\", M, N);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\", numBlocks, numThreads);\n",
    "\n",
    "    for (size_t i = 0; i < loope; i++){\n",
    "        mvp_kernel<<<numBlocks, numThreads>>>(A, x, y, M, N);\n",
    "    }\n",
    "\n",
    "    //barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    printf(\"\\n--- Output Vector Y (m=%lu) ---\\n\", M);\n",
    "    \n",
    "    // Print first 3 elements\n",
    "    // This will trigger Device-to-Host page faults/migration for y\n",
    "    printf(\"First 3 elements:\\n\");\n",
    "    for(size_t i = 0; i < 3 && i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "    // Print last 3 elements\n",
    "    printf(\"\\nLast 3 elements:\\n\");\n",
    "    size_t start_index = (M > 3) ? (M - 3) : 0; \n",
    "    for(size_t i = start_index; i < M; i++) {\n",
    "        printf(\"y[%lu] = %f\\n\", i, y[i]);\n",
    "    }\n",
    "\n",
    "\n",
    "    // error checking routine here \n",
    "    // This will trigger Device-to-Host page faults/migration for A and x\n",
    "    printf(\"\\nStarting error check...\\n\");\n",
    "    size_t err_count = 0;\n",
    "    for (size_t i = 0; i < M; i++){ \n",
    "        float correct_y = 0.0f;\n",
    "        for (size_t j = 0; j < N; j++) {\n",
    "            correct_y += A[i * N + j] * x[j];\n",
    "        }\n",
    "        float epsilon = 1e-5f; // A small tolerance for floating point errors\n",
    "        if (fabsf(y[i] - correct_y) > epsilon) {\n",
    "            err_count++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Error count (CUDA program): %lu\\n\", err_count);\n",
    "\n",
    "    // Free memory \n",
    "    cudaFree(A);\n",
    "    cudaFree(x);\n",
    "    cudaFree(y);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0739216-6416-467a-801d-a2d055a9d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_mvp7.cu -o CUDA_mvp7 -lm -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767ea10-5f79-4b71-b20e-11379b1275a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_mvp7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d41f70-9238-4a85-af6f-42b0575c5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_mvp7 ./CUDA_mvp7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
